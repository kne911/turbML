{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "In Section AE.1 in the ebook, the NN model is trained on the normal Reynolds stresses\n",
    "in the boundary layer flow and then the Reynolds stresses in the two channel\n",
    "flows ($Re_τ$ = 550 and $Re_τ$ = 5 200) are predicted. Do it the other way around:\n",
    "use one of the channel flows as training data and predict the Reynolds stresses for\n",
    "the other two flows. You need probably change the limits on y+ and ∂U+/∂y+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from LoadData import *\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch \n",
    "import sys \n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from random import randrange\n",
    "from joblib import dump, load\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.interactive(True)\n",
    "plt.close('all')\n",
    "# Create output path\n",
    "outputPath = 'Output/'\n",
    "Path(\"Output\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get DNS/LES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning data from: FullyDevelopedChannel_Re550. Min yplus: 20. Max yplus: 2200\n",
      "Returning c = [c0, c2]\n"
     ]
    }
   ],
   "source": [
    "y_DNS, yplus_DNS, u_DNS, uu_DNS, vv_DNS, ww_DNS, uv_DNS, k_DNS, eps_DNS, dudy_DNS = GetInputData('FullyDevelopedChannel_Re550', 20, 2200)\n",
    "c, a11_DNS, a33_DNS = GetC0andC2(k_DNS, eps_DNS, dudy_DNS, uu_DNS, vv_DNS, ww_DNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m fig1,ax1 \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplots_adjust(left\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m,bottom\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m ax1\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39ma11_DNS\u001b[38;5;241m+\u001b[39ma33_DNS,yplus_DNS, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeural Network\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$2a_\u001b[39m\u001b[38;5;132;01m{11}\u001b[39;00m\u001b[38;5;124m+a_\u001b[39m\u001b[38;5;132;01m{33}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$y^+$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/axes/_axes.py:4655\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4653\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m   4654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n\u001b[0;32m-> 4655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4658\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal.classic_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   4659\u001b[0m          mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = True\n",
    "\n",
    "if plot:\n",
    "    ########################## 2*a11_DNS+a33_DNS\n",
    "    fig1,ax1 = plt.subplots()\n",
    "    plt.subplots_adjust(left=0.20,bottom=0.20)\n",
    "    ax1.scatter(2*a11_DNS+a33_DNS,yplus_DNS, marker=\"o\", s=10, c=\"red\", label=\"Neural Network\")\n",
    "    plt.xlabel(\"$2a_{11}+a_{33}$\")\n",
    "    plt.ylabel(\"$y^+$\")\n",
    "    plt.legend(loc=\"best\",fontsize=12)\n",
    "    #plt.savefig('Output/2a11_DNS+a33_DNS-dudy2-and-tau-2-hidden-9-yplus-2200-dudy-min-eq.4e-4-scale-with-k-eps-units-BL.png')\n",
    "\n",
    "\n",
    "    prod_DNS_1 = -uv_DNS*dudy_DNS\n",
    "\n",
    "    ########################## k-bal\n",
    "    fig1,ax1 = plt.subplots()\n",
    "    plt.subplots_adjust(left=0.20,bottom=0.20)\n",
    "    #ax1.plot(yplus_DNS_uu,prod_DNS, 'b-', label=\"prod\")\n",
    "    ax1.plot(yplus_DNS,prod_DNS_1, 'b-', label=\"$-\\\\overline{u'v'} \\\\partial U/\\\\partial y$\")\n",
    "    ax1.plot(yplus_DNS,-eps_DNS,'r--', label=\"-diss\")\n",
    "    plt.axis([0,200,0,0.3])\n",
    "    plt.ylabel(\"$y^+$\")\n",
    "    plt.legend(loc=\"best\",fontsize=12)\n",
    "    #plt.savefig('Output/prod-diss-DNS-dudy2-and-tau-2-hidden-9-yplus-2200-dudy-min-eq.4e-4-scale-with-ustar-and-nu-BL.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose the target vector to make it a column vector  \n",
    "y = c.transpose()\n",
    "tau_DNS = k_DNS/eps_DNS\n",
    "dudy_squared_DNS = (dudy_DNS**2)\n",
    "# scale with k and eps \n",
    "# dudy [1/T]\n",
    "# dudy**2 [1/T**2]\n",
    "T = tau_DNS\n",
    "dudy_squared_DNS_scaled = dudy_squared_DNS*T**2\n",
    "dudy_DNS_inv = 1/dudy_DNS/T\n",
    "# re-shape\n",
    "dudy_squared_DNS_scaled = dudy_squared_DNS_scaled.reshape(-1,1)\n",
    "dudy_DNS_inv_scaled = dudy_DNS_inv.reshape(-1,1)\n",
    "# use MinMax scaler\n",
    "#scaler_dudy2 = StandardScaler()\n",
    "#scaler_tau = StandardScaler()\n",
    "scaler_dudy2 = MinMaxScaler()\n",
    "scaler_dudy = MinMaxScaler()\n",
    "X=np.zeros((len(dudy_DNS),2))\n",
    "X[:,0] = scaler_dudy2.fit_transform(dudy_squared_DNS_scaled)[:,0]\n",
    "X[:,1] = scaler_dudy.fit_transform(dudy_DNS_inv_scaled)[:,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the feature matrix and target vector into training and validation sets\n",
    "# test_size=0.2 means we reserve 20% of the data for validation\n",
    "# random_state=42 is a fixed seed for the random number generator, ensuring reproducibility\n",
    "\n",
    "random_state = randrange(100)\n",
    "\n",
    "indices = np.arange(len(X))\n",
    "\n",
    "# Set test size to 0 for now\n",
    "X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(X, y, indices,test_size = 0.1 ,shuffle=True,random_state=42)\n",
    "\n",
    "# convert the numpy arrays to PyTorch tensors with float32 data type\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "my_batch_size = 5\n",
    "# create PyTorch datasets and dataloaders for the training and validation sets\n",
    "# a TensorDataset wraps the feature and target tensors into a single dataset\n",
    "# a DataLoader loads the data in batches and shuffles the batches if shuffle=True\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=my_batch_size)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=my_batch_size)\n",
    "\n",
    "c_0_DNS = c[0, :]\n",
    "c_2_DNS = c[1, :]\n",
    "\n",
    "# Assertions instead of plotting the data\n",
    "for i, k in enumerate(index_train):\n",
    "    assert abs(c_0_DNS[k] - train_dataset[i][1][0].numpy()) < 0.000001, \"Training set for C0 differs from input values!\"\n",
    "    assert abs(c_2_DNS[k] - train_dataset[i][1][1].numpy()) < 0.000001, \"Training set for C2 differs from input values!\"\n",
    "    \n",
    "for i, k in enumerate(index_test):\n",
    "    assert abs(c_0_DNS[k] - test_dataset[i][1][0].numpy()) < 0.000001, \"Test set for C0 differs from input values!\"\n",
    "    assert abs(c_2_DNS[k] - test_dataset[i][1][1].numpy()) < 0.000001, \"Test set for C2 differs from input values!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Loss: 8.4295\n",
      "Epoch [20], Loss: 5.8146\n",
      "Epoch [30], Loss: 4.7411\n",
      "Epoch [40], Loss: 3.9198\n",
      "Epoch [50], Loss: 3.3218\n",
      "Epoch [60], Loss: 2.9887\n",
      "Epoch [70], Loss: 2.8770\n",
      "Epoch [80], Loss: 2.8764\n",
      "Epoch [90], Loss: 2.8758\n",
      "Epoch [100], Loss: 2.8751\n",
      "Epoch [110], Loss: 2.8742\n",
      "Epoch [120], Loss: 2.8736\n",
      "Epoch [130], Loss: 2.8733\n",
      "Epoch [140], Loss: 2.8729\n",
      "Epoch [150], Loss: 2.8724\n",
      "Epoch [160], Loss: 2.8720\n",
      "Epoch [170], Loss: 2.8716\n",
      "Epoch [180], Loss: 2.8712\n",
      "Epoch [190], Loss: 2.8706\n",
      "Epoch [200], Loss: 2.8699\n",
      "Epoch [210], Loss: 2.8692\n",
      "Epoch [220], Loss: 2.8687\n",
      "Epoch [230], Loss: 2.8683\n",
      "Epoch [240], Loss: 2.8682\n",
      "Epoch [250], Loss: 2.8684\n",
      "Epoch [260], Loss: 2.8686\n",
      "Epoch [270], Loss: 2.8688\n",
      "Epoch [280], Loss: 2.8691\n",
      "Epoch [290], Loss: 2.8697\n",
      "Epoch [300], Loss: 2.8701\n",
      "Epoch [310], Loss: 2.8707\n",
      "Epoch [320], Loss: 2.8711\n",
      "Epoch [330], Loss: 2.8716\n",
      "Epoch [340], Loss: 2.8720\n",
      "Epoch [350], Loss: 2.8724\n",
      "Epoch [360], Loss: 2.8728\n",
      "Epoch [370], Loss: 2.8735\n",
      "Epoch [380], Loss: 2.8738\n",
      "Epoch [390], Loss: 2.8741\n",
      "Epoch [400], Loss: 2.8745\n",
      "Epoch [410], Loss: 2.8748\n",
      "Epoch [420], Loss: 2.8750\n",
      "Epoch [430], Loss: 2.8754\n",
      "Epoch [440], Loss: 2.8758\n",
      "Epoch [450], Loss: 2.8761\n",
      "Epoch [460], Loss: 2.8763\n",
      "Epoch [470], Loss: 2.8765\n",
      "Epoch [480], Loss: 2.8766\n",
      "Epoch [490], Loss: 2.8766\n",
      "Epoch [500], Loss: 2.8767\n",
      "Epoch [510], Loss: 2.8768\n",
      "Epoch [520], Loss: 2.8768\n",
      "Epoch [530], Loss: 2.8768\n",
      "Epoch [540], Loss: 2.8768\n",
      "Epoch [550], Loss: 2.8766\n",
      "Epoch [560], Loss: 2.8765\n",
      "Epoch [570], Loss: 2.8762\n",
      "Epoch [580], Loss: 2.8763\n",
      "Epoch [590], Loss: 2.8766\n",
      "Epoch [600], Loss: 2.8767\n",
      "Epoch [610], Loss: 2.8766\n",
      "Epoch [620], Loss: 2.8765\n",
      "Epoch [630], Loss: 2.8764\n",
      "Epoch [640], Loss: 2.8762\n",
      "Epoch [650], Loss: 2.8758\n",
      "Epoch [660], Loss: 2.8754\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[0;32m---> 73\u001b[0m     loss \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(closure)  \u001b[38;5;66;03m# Optimize model parameters\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Print the loss every 10th epoch\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/sgd.py:112\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 112\u001b[0m         loss \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    115\u001b[0m     params: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[127], line 20\u001b[0m, in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero out gradients\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X)  \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, y)  \u001b[38;5;66;03m# Compute mean squared error loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Calculate the L1 regularization term\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[127], line 10\u001b[0m, in \u001b[0;36mnn_turbML.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, linear \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears):\n\u001b[0;32m---> 10\u001b[0m         x \u001b[38;5;241m=\u001b[39m linear(x)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# No activation function on output layer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class nn_turbML(nn.Module):\n",
    "\n",
    "    def __init__(self, nNodes):\n",
    "        super(nn_turbML, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(2, nNodes), nn.Linear(nNodes, nNodes), nn.Linear(nNodes,2)])\n",
    "        self.actFunction = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            # No activation function on output layer\n",
    "            if i == len(self.linears)-1:\n",
    "                break\n",
    "            x = self.actFunction(x)\n",
    "        return x\n",
    "\n",
    "def closure():\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # Zero out gradients\n",
    "        outputs = model(X)  # Forward pass through the model\n",
    "        loss = loss_fn(outputs, y)  # Compute mean squared error loss\n",
    "\n",
    "        # Calculate the L1 regularization term\n",
    "        l1_regularization = torch.tensor(0.)\n",
    "        for param in model.parameters():\n",
    "            l1_regularization += torch.norm(param, p=1)  # L1 norm of model parameters\n",
    "\n",
    "        # Add the L1 regularization term to the loss\n",
    "        loss += lambda_l1 * l1_regularization  # Add L1 regularization to the loss\n",
    "        loss.backward()  # Compute gradients\n",
    "    return loss\n",
    "  \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    global pred_numpy,pred1,size1\n",
    "    size = len(dataloader.dataset)\n",
    "    size1 = size\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    print('in test_loop: len(dataloader)',len(dataloader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "#transform from tensor to numpy\n",
    "            pred_numpy = pred.detach().numpy()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    print(f\"Avg loss: {test_loss:>.2e} \\n\")\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Instantiate a neural network\n",
    "model = nn_turbML(50)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# Set up hyperparameters\n",
    "learning_rate = 1e-1\n",
    "max_epochs = 10000\n",
    "lambda_l1 = 0.1\n",
    "prev_loss = float('inf') \n",
    "\n",
    "# Choose loss function, check out https://pytorch.org/docs/stable/optim.html for more info\n",
    "# In this case we choose Stocastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    loss = optimizer.step(closure)  # Optimize model parameters\n",
    "\n",
    "    # Print the loss every 10th epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}], Loss: {closure().item():.4f}\")\n",
    "\n",
    "    loss_change = prev_loss - loss.item()\n",
    "    prev_loss = loss.item()\n",
    "\n",
    "#test_loss = test_loop(test_loader, neural_net, loss_fn)\n",
    "\n",
    "\n",
    "preds = model(X_test_tensor)\n",
    "\n",
    "print(f\"{'time ML: '}{time.time()-start_time:.2e}\")\n",
    "\n",
    "#transform from tensor to numpy\n",
    "c_NN = preds.detach().numpy()\n",
    " \n",
    "c_NN_old = c_NN\n",
    "\n",
    "c0=c_NN[:,0]\n",
    "c2=c_NN[:,1]\n",
    "\n",
    "c0_std=np.std(c0-c0_DNS[index_test])/(np.mean(c0.flatten()**2))**0.5\n",
    "c2_std=np.std(c2-c2_DNS[index_test])/(np.mean(c2.flatten()**2))**0.5\n",
    "\n",
    "print('\\nc0_error_std: ',c0_std)\n",
    "print('\\nc2_error_std: ',c2_std)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
