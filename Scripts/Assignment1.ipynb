{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "In Section AE.1 in the ebook, the NN model is trained on the normal Reynolds stresses\n",
    "in the boundary layer flow and then the Reynolds stresses in the two channel\n",
    "flows ($Re_τ$ = 550 and $Re_τ$ = 5 200) are predicted. Do it the other way around:\n",
    "use one of the channel flows as training data and predict the Reynolds stresses for\n",
    "the other two flows. You need probably change the limits on y+ and ∂U+/∂y+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from LoadData import *\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch \n",
    "import sys \n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from random import randrange\n",
    "from joblib import dump, load\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.interactive(True)\n",
    "plt.close('all')\n",
    "# Create output path\n",
    "outputPath = 'Output/'\n",
    "Path(\"Output\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DNS/LES data for channel flow, $Re = 5200$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning data from: BoundaryLayer. Min yplus: 20. Max yplus: 2200\n",
      "Returning c = [c0, c2], a11 and a33\n"
     ]
    }
   ],
   "source": [
    "minYplus = 20\n",
    "maxYplus = 2200\n",
    "\n",
    "y_DNS, yplus_DNS, u_DNS, uu_DNS, vv_DNS, ww_DNS, uv_DNS, k_DNS, eps_DNS, dudy_DNS = GetInputData('BoundaryLayer', minYplus, maxYplus)\n",
    "c, a11_DNS, a33_DNS = GetC0andC2(k_DNS, eps_DNS, dudy_DNS, uu_DNS, vv_DNS, ww_DNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot to check that dissipation and production are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = False\n",
    "\n",
    "if plot:\n",
    "    ########################## 2*a11_DNS+a33_DNS\n",
    "    fig1,ax1 = plt.subplots()\n",
    "    plt.subplots_adjust(left=0.20,bottom=0.20)\n",
    "    ax1.scatter(2*a11_DNS+a33_DNS,yplus_DNS, marker=\"o\", s=10, c=\"red\", label=\"Neural Network\")\n",
    "    plt.xlabel(\"$2a_{11}+a_{33}$\")\n",
    "    plt.ylabel(\"$y^+$\")\n",
    "    plt.legend(loc=\"best\",fontsize=12)\n",
    "    #plt.savefig('Output/2a11_DNS+a33_DNS-dudy2-and-tau-2-hidden-9-yplus-2200-dudy-min-eq.4e-4-scale-with-k-eps-units-BL.png')\n",
    "\n",
    "\n",
    "    prod_DNS_1 = -uv_DNS*dudy_DNS\n",
    "\n",
    "    ########################## k-bal\n",
    "    fig1,ax1 = plt.subplots()\n",
    "    plt.subplots_adjust(left=0.20,bottom=0.20)\n",
    "    #ax1.plot(yplus_DNS_uu,prod_DNS, 'b-', label=\"prod\")\n",
    "    ax1.plot(yplus_DNS,prod_DNS_1, 'b-', label=\"$-\\\\overline{u'v'} \\\\partial U/\\\\partial y$\")\n",
    "    ax1.plot(yplus_DNS,-eps_DNS,'r--', label=\"-diss\")\n",
    "    #plt.axis([0,200,0,0.3])\n",
    "    plt.xlabel(\"$y^+$\")\n",
    "    plt.legend(loc=\"best\",fontsize=12)\n",
    "    #plt.savefig('Output/prod-diss-DNS-dudy2-and-tau-2-hidden-9-yplus-2200-dudy-min-eq.4e-4-scale-with-ustar-and-nu-BL.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate $\\frac{du}{dy}$ and $\\frac{du}{dy}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose the target vector to make it a column vector  \n",
    "y = c.transpose()\n",
    "tau_DNS = abs(k_DNS/eps_DNS)\n",
    "dudy_squared_DNS = (dudy_DNS**2)\n",
    "# scale with k and eps \n",
    "# dudy [1/T]\n",
    "# dudy**2 [1/T**2]\n",
    "T = tau_DNS\n",
    "dudy_squared_DNS_scaled = dudy_squared_DNS*T**2\n",
    "dudy_DNS_inv = 1/dudy_DNS/T\n",
    "# re-shape\n",
    "dudy_squared_DNS_scaled = dudy_squared_DNS_scaled.reshape(-1,1)\n",
    "dudy_DNS_inv_scaled = dudy_DNS_inv.reshape(-1,1)\n",
    "# use MinMax scaler\n",
    "#scaler_dudy2 = StandardScaler()\n",
    "#scaler_tau = StandardScaler()\n",
    "scaler_dudy2 = MinMaxScaler()\n",
    "scaler_dudy = MinMaxScaler()\n",
    "X=np.zeros((len(dudy_DNS),2))\n",
    "X[:,0] = scaler_dudy2.fit_transform(dudy_squared_DNS_scaled)[:,0]\n",
    "X[:,1] = scaler_dudy.fit_transform(dudy_DNS_inv_scaled)[:,0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the feature matrix and target vector into training and validation sets\n",
    "# test_size=0.2 means we reserve 20% of the data for validation\n",
    "# random_state=42 is a fixed seed for the random number generator, ensuring reproducibility\n",
    "\n",
    "random_state = randrange(100)\n",
    "\n",
    "indices = np.arange(len(X))\n",
    "\n",
    "# Set test size to 0 for now\n",
    "X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(X, y, indices,test_size = 0.1 ,shuffle=True,random_state=42)\n",
    "\n",
    "# convert the numpy arrays to PyTorch tensors with float32 data type\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "my_batch_size = 5\n",
    "# create PyTorch datasets and dataloaders for the training and validation sets\n",
    "# a TensorDataset wraps the feature and target tensors into a single dataset\n",
    "# a DataLoader loads the data in batches and shuffles the batches if shuffle=True\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=my_batch_size)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=my_batch_size)\n",
    "\n",
    "c_0_DNS = c[0, :]\n",
    "c_2_DNS = c[1, :]\n",
    "\n",
    "# Assertions instead of plotting the data\n",
    "for i, k in enumerate(index_train):\n",
    "    assert abs(c_0_DNS[k] - train_dataset[i][1][0].numpy()) < 0.000001, \"Training set for C0 differs from input values!\"\n",
    "    assert abs(c_2_DNS[k] - train_dataset[i][1][1].numpy()) < 0.000001, \"Training set for C2 differs from input values!\"\n",
    "    \n",
    "for i, k in enumerate(index_test):\n",
    "    assert abs(c_0_DNS[k] - test_dataset[i][1][0].numpy()) < 0.000001, \"Test set for C0 differs from input values!\"\n",
    "    assert abs(c_2_DNS[k] - test_dataset[i][1][1].numpy()) < 0.000001, \"Test set for C2 differs from input values!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 5.3190\n",
      "Epoch [20/1000], Loss: 4.3486\n",
      "Epoch [30/1000], Loss: 3.5352\n",
      "Epoch [40/1000], Loss: 2.8437\n",
      "Epoch [50/1000], Loss: 2.2596\n",
      "Epoch [60/1000], Loss: 1.7925\n",
      "Epoch [70/1000], Loss: 1.5135\n",
      "Epoch [80/1000], Loss: 2.4712\n",
      "Epoch [90/1000], Loss: 2.3199\n",
      "Epoch [100/1000], Loss: 3.4803\n",
      "Epoch [110/1000], Loss: 2.0590\n",
      "Epoch [120/1000], Loss: 3.3317\n",
      "Epoch [130/1000], Loss: 2.3801\n",
      "Epoch [140/1000], Loss: 2.7496\n",
      "Epoch [150/1000], Loss: 2.4860\n",
      "Epoch [160/1000], Loss: 2.5961\n",
      "Epoch [170/1000], Loss: 2.4257\n",
      "Epoch [180/1000], Loss: 2.4889\n",
      "Epoch [190/1000], Loss: 2.4065\n",
      "Epoch [200/1000], Loss: 2.3874\n",
      "Epoch [210/1000], Loss: 2.3496\n",
      "Epoch [220/1000], Loss: 2.3279\n",
      "Epoch [230/1000], Loss: 2.2986\n",
      "Epoch [240/1000], Loss: 2.2661\n",
      "Epoch [250/1000], Loss: 2.2384\n",
      "Epoch [260/1000], Loss: 2.2111\n",
      "Epoch [270/1000], Loss: 2.1840\n",
      "Epoch [280/1000], Loss: 2.1476\n",
      "Epoch [290/1000], Loss: 2.1336\n",
      "Epoch [300/1000], Loss: 2.1029\n",
      "Epoch [310/1000], Loss: 2.0675\n",
      "Epoch [320/1000], Loss: 2.0452\n",
      "Epoch [330/1000], Loss: 2.0133\n",
      "Epoch [340/1000], Loss: 1.9877\n",
      "Epoch [350/1000], Loss: 1.9559\n",
      "Epoch [360/1000], Loss: 1.9290\n",
      "Epoch [370/1000], Loss: 1.9005\n",
      "Epoch [380/1000], Loss: 1.8774\n",
      "Epoch [390/1000], Loss: 1.8521\n",
      "Epoch [400/1000], Loss: 1.8347\n",
      "Epoch [410/1000], Loss: 1.8142\n",
      "Epoch [420/1000], Loss: 1.7959\n",
      "Epoch [430/1000], Loss: 1.7781\n",
      "Epoch [440/1000], Loss: 1.7645\n",
      "Epoch [450/1000], Loss: 1.7489\n",
      "Epoch [460/1000], Loss: 1.7361\n",
      "Epoch [470/1000], Loss: 1.7260\n",
      "Epoch [480/1000], Loss: 1.7141\n",
      "Epoch [490/1000], Loss: 1.7006\n",
      "Epoch [500/1000], Loss: 1.6857\n",
      "Epoch [510/1000], Loss: 1.6758\n",
      "Epoch [520/1000], Loss: 1.6658\n",
      "Epoch [530/1000], Loss: 1.6517\n",
      "Epoch [540/1000], Loss: 1.6370\n",
      "Epoch [550/1000], Loss: 1.6268\n",
      "Epoch [560/1000], Loss: 1.6139\n",
      "Epoch [570/1000], Loss: 1.5967\n",
      "Epoch [580/1000], Loss: 1.5832\n",
      "Epoch [590/1000], Loss: 1.5686\n",
      "Epoch [600/1000], Loss: 1.5501\n",
      "Epoch [610/1000], Loss: 1.5310\n",
      "Epoch [620/1000], Loss: 1.5127\n",
      "Epoch [630/1000], Loss: 1.4952\n",
      "Epoch [640/1000], Loss: 1.4749\n",
      "Epoch [650/1000], Loss: 1.4519\n",
      "Epoch [660/1000], Loss: 1.4354\n",
      "Epoch [670/1000], Loss: 1.4112\n",
      "Epoch [680/1000], Loss: 1.3935\n",
      "Epoch [690/1000], Loss: 1.3798\n",
      "Epoch [700/1000], Loss: 1.3680\n",
      "Epoch [710/1000], Loss: 1.3535\n",
      "Epoch [720/1000], Loss: 1.3465\n",
      "Epoch [730/1000], Loss: 1.3393\n",
      "Epoch [740/1000], Loss: 1.3255\n",
      "Epoch [750/1000], Loss: 1.3226\n",
      "Epoch [760/1000], Loss: 1.3122\n",
      "Epoch [770/1000], Loss: 1.3108\n",
      "Epoch [780/1000], Loss: 1.3033\n",
      "Epoch [790/1000], Loss: 1.3055\n",
      "Epoch [800/1000], Loss: 1.2996\n",
      "Epoch [810/1000], Loss: 1.2999\n",
      "Epoch [820/1000], Loss: 1.2963\n",
      "Epoch [830/1000], Loss: 1.2972\n",
      "Epoch [840/1000], Loss: 1.2926\n",
      "Epoch [850/1000], Loss: 1.2945\n",
      "Epoch [860/1000], Loss: 1.2932\n",
      "Epoch [870/1000], Loss: 1.2958\n",
      "Epoch [880/1000], Loss: 1.2955\n",
      "Epoch [890/1000], Loss: 1.2948\n",
      "Epoch [900/1000], Loss: 1.2927\n",
      "Epoch [910/1000], Loss: 1.2924\n",
      "Epoch [920/1000], Loss: 1.2908\n",
      "Epoch [930/1000], Loss: 1.2909\n",
      "Epoch [940/1000], Loss: 1.2896\n",
      "Epoch [950/1000], Loss: 1.2887\n",
      "Epoch [960/1000], Loss: 1.2878\n",
      "Epoch [970/1000], Loss: 1.2887\n",
      "Epoch [980/1000], Loss: 1.2866\n",
      "Epoch [990/1000], Loss: 1.2888\n",
      "Epoch [1000/1000], Loss: 1.2878\n",
      "time ML: 1.33e+01\n",
      "\n",
      "c0_error_std:  0.0065246345135903635\n",
      "\n",
      "c2_error_std:  0.0026967991078740033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class nn_turbML(nn.Module):\n",
    "\n",
    "    def __init__(self, nNodes):\n",
    "        super(nn_turbML, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(2, nNodes), nn.Linear(nNodes, nNodes), nn.Linear(nNodes,2)])\n",
    "        self.actFunction = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            # No activation function on output layer\n",
    "            if i == len(self.linears)-1:\n",
    "                break\n",
    "            x = self.actFunction(x)\n",
    "        return x\n",
    "\n",
    "def closure():\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # Zero out gradients\n",
    "        outputs = model(X)  # Forward pass through the model\n",
    "        loss = loss_fn(outputs, y)  # Compute mean squared error loss\n",
    "\n",
    "        # Calculate the L1 regularization term\n",
    "        l1_regularization = torch.tensor(0.)\n",
    "        for param in model.parameters():\n",
    "            l1_regularization += torch.norm(param, p=1)  # L1 norm of model parameters\n",
    "\n",
    "        # Add the L1 regularization term to the loss\n",
    "        loss += lambda_l1 * l1_regularization  # Add L1 regularization to the loss\n",
    "        loss.backward()  # Compute gradients\n",
    "    return loss\n",
    "  \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    global pred_numpy,pred1,size1\n",
    "    size = len(dataloader.dataset)\n",
    "    size1 = size\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    print('in test_loop: len(dataloader)',len(dataloader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "#transform from tensor to numpy\n",
    "            pred_numpy = pred.detach().numpy()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    print(f\"Avg loss: {test_loss:>.2e} \\n\")\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Instantiate a neural network\n",
    "model = nn_turbML(100)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# Set up hyperparameters\n",
    "learning_rate = 1e-1\n",
    "max_epochs = 1000\n",
    "lambda_l1 = 0.005\n",
    "prev_loss = float('inf') \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    loss = optimizer.step(closure)  # Optimize model parameters\n",
    "\n",
    "    # Print the loss every 10th epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{max_epochs}], Loss: {closure().item():.4f}\")\n",
    "\n",
    "    loss_change = prev_loss - loss.item()\n",
    "    prev_loss = loss.item()\n",
    "\n",
    "#test_loss = test_loop(test_loader, neural_net, loss_fn)\n",
    "\n",
    "\n",
    "preds = model(X_test_tensor)\n",
    "\n",
    "print(f\"{'time ML: '}{time.time()-start_time:.2e}\")\n",
    "\n",
    "#transform from tensor to numpy\n",
    "c_NN = preds.detach().numpy()\n",
    " \n",
    "c_NN_old = c_NN\n",
    "\n",
    "c0=c_NN[:,0]\n",
    "c2=c_NN[:,1]\n",
    "\n",
    "c0_std=np.std(c0-c_0_DNS[index_test])/(np.mean(c0.flatten()**2))**0.5\n",
    "c2_std=np.std(c2-c_2_DNS[index_test])/(np.mean(c2.flatten()**2))**0.5\n",
    "\n",
    "print('\\nc0_error_std: ',c0_std)\n",
    "print('\\nc2_error_std: ',c2_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate with boundary layer flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning data from: BoundaryLayer. Min yplus: 20. Max yplus: 2200\n",
      "Returning c = [c0, c2], a11 and a33\n",
      "\n",
      "c0_error_std:  0.008826484929120105\n",
      "\n",
      "c2_error_std:  0.0037075655742879673\n"
     ]
    }
   ],
   "source": [
    "y_DNS, yplus_DNS, u_DNS, uu_DNS, vv_DNS, ww_DNS, uv_DNS, k_DNS, eps_DNS, dudy_DNS = GetInputData('BoundaryLayer', minYplus, maxYplus)\n",
    "c, a11_DNS, a33_DNS = GetC0andC2(k_DNS, eps_DNS, dudy_DNS, uu_DNS, vv_DNS, ww_DNS)\n",
    "\n",
    "c_0_DNS_validation = c[0, :]\n",
    "c_2_DNS_validation = c[1, :]\n",
    "# transpose the target vector to make it a column vector  \n",
    "y = c.transpose()\n",
    "tau_DNS = abs(k_DNS/eps_DNS)\n",
    "dudy_squared_DNS = (dudy_DNS**2)\n",
    "# scale with k and eps \n",
    "# dudy [1/T]\n",
    "# dudy**2 [1/T**2]\n",
    "T = tau_DNS\n",
    "dudy_squared_DNS_scaled = dudy_squared_DNS*T**2\n",
    "dudy_DNS_inv = 1/dudy_DNS/T\n",
    "# re-shape\n",
    "dudy_squared_DNS_scaled = dudy_squared_DNS_scaled.reshape(-1,1)\n",
    "dudy_DNS_inv_scaled = dudy_DNS_inv.reshape(-1,1)\n",
    "# use MinMax scaler\n",
    "#scaler_dudy2 = StandardScaler()\n",
    "#scaler_tau = StandardScaler()\n",
    "\n",
    "## We have to use the same scaler as for the training data set\n",
    "X=np.zeros((len(dudy_DNS),2))\n",
    "X[:,0] = scaler_dudy2.transform(dudy_squared_DNS_scaled)[:,0]\n",
    "X[:,1] = scaler_dudy.transform(dudy_DNS_inv_scaled)[:,0]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "predictions_Boundary = model(X)\n",
    "\n",
    "c_NN = predictions_Boundary.detach().numpy()\n",
    " \n",
    "c_NN_old = c_NN\n",
    "\n",
    "c0=c_NN[:,0]\n",
    "c2=c_NN[:,1]\n",
    "\n",
    "c0_std=np.std(c0-c_0_DNS_validation)/(np.mean(c0.flatten()**2))**0.5\n",
    "c2_std=np.std(c2-c_2_DNS_validation)/(np.mean(c2.flatten()**2))**0.5\n",
    "\n",
    "print('\\nc0_error_std: ',c0_std)\n",
    "print('\\nc2_error_std: ',c2_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate with channel flow, $Re = 550$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning data from: FullyDevelopedChannel_Re550. Min yplus: 30. Max yplus: 3000\n",
      "Returning c = [c0, c2], a11 and a33\n",
      "\n",
      "c0_error_std:  4.082049648244015\n",
      "\n",
      "c2_error_std:  1.9704315844711013\n"
     ]
    }
   ],
   "source": [
    "y_DNS, yplus_DNS, u_DNS, uu_DNS, vv_DNS, ww_DNS, uv_DNS, k_DNS, eps_DNS, dudy_DNS = GetInputData('FullyDevelopedChannel_Re550' , 30, 3000)\n",
    "c, a11_DNS, a33_DNS = GetC0andC2(k_DNS, eps_DNS, dudy_DNS, uu_DNS, vv_DNS, ww_DNS)\n",
    "\n",
    "c_0_DNS_validation = c[0, :]\n",
    "c_2_DNS_validation = c[1, :]\n",
    "# transpose the target vector to make it a column vector  \n",
    "y = c.transpose()\n",
    "tau_DNS = abs(k_DNS/eps_DNS)\n",
    "dudy_squared_DNS = (dudy_DNS**2)\n",
    "# scale with k and eps \n",
    "# dudy [1/T]\n",
    "# dudy**2 [1/T**2]\n",
    "T = tau_DNS\n",
    "dudy_squared_DNS_scaled = dudy_squared_DNS*T**2\n",
    "dudy_DNS_inv = 1/dudy_DNS/T\n",
    "# re-shape\n",
    "dudy_squared_DNS_scaled = dudy_squared_DNS_scaled.reshape(-1,1)\n",
    "dudy_DNS_inv_scaled = dudy_DNS_inv.reshape(-1,1)\n",
    "# use MinMax scaler\n",
    "#scaler_dudy2 = StandardScaler()\n",
    "#scaler_tau = StandardScaler()\n",
    "\n",
    "## We have to use the same scaler as for the training data set\n",
    "X=np.zeros((len(dudy_DNS),2))\n",
    "X[:,0] = scaler_dudy2.transform(dudy_squared_DNS_scaled)[:,0]\n",
    "X[:,1] = scaler_dudy.transform(dudy_DNS_inv_scaled)[:,0]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "predictions_Boundary = model(X)\n",
    "\n",
    "c_NN = predictions_Boundary.detach().numpy()\n",
    " \n",
    "c_NN_old = c_NN\n",
    "\n",
    "c0=c_NN[:,0]\n",
    "c2=c_NN[:,1]\n",
    "\n",
    "c0_std=np.std(c0-c_0_DNS_validation)/(np.mean(c0.flatten()**2))**0.5\n",
    "c2_std=np.std(c2-c_2_DNS_validation)/(np.mean(c2.flatten()**2))**0.5\n",
    "\n",
    "print('\\nc0_error_std: ',c0_std)\n",
    "print('\\nc2_error_std: ',c2_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
